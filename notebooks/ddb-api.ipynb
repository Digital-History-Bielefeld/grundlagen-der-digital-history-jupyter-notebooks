{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DLFqCYQVFcC"
   },
   "source": [
    "# Interacting with the German Digital Library API\n",
    "\n",
    "*Notebook based on https://github.com/Digital-History-Bielefeld/llm-supported-workflow-for-processing-faulty-ocr*\n",
    "\n",
    "This notebook shows how to interact with the German Digital Library's Application Programming Interface (API). An API is like a bridge that allows different software programs to talk to each other. It sets the guidelines for how they can request and share information. Think of it as a menu in a restaurant: it tells you what you can order and how to ask for it, without needing to know how the kitchen prepares the food. APIs make it easier for different apps and services to work together smoothly. Consider this Jupyter Notebook (the environment where you are reading right now) as an app, that exchanges information using the German Digital Library API. The programming code is written in Python, a programming language that aims to be easily understandable, even for inexperienced programmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German Historical Newspaper Portal\n",
    "\n",
    "German historical newspapers from the German Digital Library can be accessed via the DDB-API. This API is open access and allows to query the Historical Newspapers available in the German Newspaper Portal ([Deutsches Zeitungsportal](https://www.deutsche-digitale-bibliothek.de/newspaper)). An instruction, provided by the German Newspaper Portal (from Karl Krägerlin), can be found [here](https://deepnote.com/app/karl-kragelin-b83c/Zeitungsportal-API-d9224dda-8e26-4b35-a6d7-40e9507b1151). Using the DDB API in a Python app (as we will do here) is easy if you use the `ddbapi` Python package. A Python package is like a collection of tools that you can use in your program. Imagine having a toolbox with different tools for various tasks, like a hammer, a screwdriver, and pliers. A Python package contains many useful functions and modules that help you perform specific tasks in your code more easily and quickly. Instead of writing everything from scratch, you can use a package that already has the functions you need. This makes programming more efficient and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17844,
     "status": "ok",
     "timestamp": 1730817169999,
     "user": {
      "displayName": "C W",
      "userId": "01688883033358762912"
     },
     "user_tz": -60
    },
    "id": "f9ysGwj_KLa-",
    "outputId": "d3573726-740e-4b95-c0ae-704501fcd078"
   },
   "outputs": [],
   "source": [
    "# Install the \"Deutsche Digitale Bibliothek API\" package (ddbapi: https://pypi.org/project/ddbapi/)\n",
    "%pip install ddbapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the newspapers to be processed for case studies\n",
    "\n",
    "Every newspaper accessible through the German Digital Library has an ID provided by the Zeitschriftendatenbank (ZDB). With these IDs, we can target specific newspapers. For case studies, a relevant selection of ZDB-IDs can be saved in lists. This allows case-study-specific selections to be easily utilized in subsequent steps of the workflow. Additionally, time periods relevant to the case studies can be defined and saved in variables, enabling further refinement of the scope in subsequent steps of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzW_y_D7ffhD"
   },
   "outputs": [],
   "source": [
    "zdb_ids = [\n",
    "    # Vorwärts : Berliner Volksblatt ; das Abendblatt der Hauptstadt Deutschlands\n",
    "    \"2814128-3\"\n",
    "]\n",
    "\n",
    "periods = [\n",
    "    \"[1921-08-10T00:00:01Z TO 1921-08-12T23:59:59Z]\",\n",
    "    \"[1922-08-10T00:00:01Z TO 1922-08-12T23:59:59Z]\",\n",
    "    \"[1923-08-10T00:00:01Z TO 1923-08-12T23:59:59Z]\",\n",
    "    \"[1924-08-10T00:00:01Z TO 1924-08-12T23:59:59Z]\",\n",
    "    \"[1925-08-10T00:00:01Z TO 1925-08-12T23:59:59Z]\",\n",
    "    \"[1926-08-10T00:00:01Z TO 1926-08-12T23:59:59Z]\",\n",
    "    \"[1927-08-10T00:00:01Z TO 1927-08-12T23:59:59Z]\",\n",
    "    \"[1928-08-10T00:00:01Z TO 1928-08-12T23:59:59Z]\",\n",
    "    \"[1929-08-10T00:00:01Z TO 1929-08-12T23:59:59Z]\",\n",
    "    \"[1930-08-10T00:00:01Z TO 1930-08-12T23:59:59Z]\",\n",
    "    \"[1931-08-10T00:00:01Z TO 1931-08-12T23:59:59Z]\",\n",
    "    \"[1932-08-10T00:00:01Z TO 1932-08-12T23:59:59Z]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search terms to effectively query the German Historical Newspaper Portal\n",
    "\n",
    "Searching for specific terms or phrases retrieves only newspaper pages that include those keywords or phrases. We can define them individually or create thematic lists to use in the queries. Below, you find a selection of basic keyword lists. They may be combined to create specified keyword lists for more targeted queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of keywords centered on discussions about democracy in general\n",
    "keywords_democracy = [\n",
    "    \"demokrat*\",\n",
    "    \"volksherrschaft\",\n",
    "    \"demokratisier*\",\n",
    "    \"verfassung*\"\n",
    "]\n",
    "\n",
    "# A list of keywords specified on social democratic topoi in discourses on democracy\n",
    "keywords_social_democrats = [\n",
    "    \"sozialisier*\",\n",
    "    \"sozialistisch~\",\n",
    "    \"verfassung*\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for the DDB-API interaction\n",
    "\n",
    "Down below we define helper functions for using the DDB API. In Python, a function is like a recipe that you can use to perform a specific task. Imagine you have a recipe for making a sandwich. Every time you want a sandwich, you follow the same steps in the recipe. Similarly, a function in Python is a set of instructions that you can use whenever you need to do a particular job. You give it a name, and whenever you call that name, it runs the instructions inside. This helps you avoid repeating the same code and makes your programs easier to manage and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import the packages that we want to use for our helper functions\n",
    "import ddbapi\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(publication_dates: list[str], zdb_ids: list[str], keywords: list[str]) -> pd.DataFrame:\n",
    "    \"\"\" Get data on Newspaper-Pages from the DDB API for multiple periods and keywords. \"\"\"\n",
    "    data_frames = []\n",
    "\n",
    "    for publication_date in publication_dates:\n",
    "        for zdb_id in zdb_ids:\n",
    "            for keyword in keywords:\n",
    "                try:\n",
    "                    df = ddbapi.zp_pages(publication_date=publication_date, zdb_id=zdb_id, plainpagefulltext=keyword)\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        data_frames.append(df)\n",
    "                    else:\n",
    "                        print(f\"Warning: The result for date {publication_date}, zdb_id {zdb_id}, and keyword {keyword} is not a DataFrame.\")\n",
    "                except requests.exceptions.HTTPError as e:\n",
    "                    print(f\"HTTPError for date {publication_date}, zdb_id {zdb_id}, and keyword {keyword}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred for date {publication_date}, zdb_id {zdb_id}, and keyword {keyword}: {e}\")\n",
    "\n",
    "    if len(data_frames) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    return combined_df.drop_duplicates(subset=['page_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31920,
     "status": "ok",
     "timestamp": 1730817477469,
     "user": {
      "displayName": "C W",
      "userId": "01688883033358762912"
     },
     "user_tz": -60
    },
    "id": "5FLflpskwDq5",
    "outputId": "f9c3a18b-6f34-4768-c7a1-4cc5a1935faf"
   },
   "outputs": [],
   "source": [
    "def combine_keyword_lists(*keyword_lists: list[str]) -> list[str]:\n",
    "    \"\"\" Combines multiple lists of keywords into and removes duplicates. \"\"\"\n",
    "    # Use a set to combine all keywords and automatically remove duplicates\n",
    "    combined_set = set()\n",
    "    for keyword_list in keyword_lists:\n",
    "        combined_set.update(keyword_list)\n",
    "    # Convert the set back to a list\n",
    "    keywords_combined = list(combined_set)\n",
    "    return keywords_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions in action\n",
    "\n",
    "In the next step we will use our helper functions and receive the data in a pandas data frame. A pandas DataFrame is like a table or a spreadsheet that you can use in your programs. Imagine an Excel sheet where you have rows and columns to organize your data. Each column can have a different type of information, like names, dates, or numbers, and each row represents a different entry or record. A pandas DataFrame works the same way, allowing you to store, organize, and manipulate data easily. It's a powerful tool for handling data because you can quickly sort, filter, and analyze the information, just like you would in a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specified list of keywords by combining basic keyword lists\n",
    "keywords = combine_keyword_lists(\n",
    "    keywords_democracy,\n",
    "    keywords_social_democrats\n",
    ")\n",
    "\n",
    "# Get data on newspapers to query\n",
    "pages = get_pages(\n",
    "    periods,\n",
    "    zdb_ids,\n",
    "    keywords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of entries and the first few rows of the DataFrame\n",
    "print(f\"Number of entries: {len(pages)}\")\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing keyword findings through frequency charts\n",
    "\n",
    "First, we extract information on the count of keyword search hits for each day from the above data frame. We store that information in a new pandas DataFrame to get a first impression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'publication_date' column to datetime format (if not already)\n",
    "pages['publication_date'] = pd.to_datetime(pages['publication_date'])\n",
    "\n",
    "# Extract year, month, and day from the 'publication_date' column and create new columns for each\n",
    "pages['year'] = pages['publication_date'].dt.year\n",
    "pages['month'] = pages['publication_date'].dt.month\n",
    "pages['day'] = pages['publication_date'].dt.day\n",
    "\n",
    "# Create a new DataFrame with the counts of pages per day\n",
    "daily_counts = pages.groupby(['year', 'month', 'day']).size().reset_index(name='count')\n",
    "\n",
    "# Display the first few rows of the \"daily_counts\" DataFrame\n",
    "daily_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced frequency based visualizations we use more Python packages called `seaborn` and `matplotlib`. These need to be installed the same way that we already did with the `ddbapi` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the `seaborn` and `matplotlib` packages for visualization\n",
    "%pip install seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import the new packages that we want to use for our helper functions\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linegraph_search_hits(df: pd.DataFrame, keywords: list[str], time_granularity: str):\n",
    "    ''' Plots keyword search hits over time with specified granularity (pass 'day', 'month', or 'year'). '''\n",
    "\n",
    "    xlabel = time_granularity.capitalize()\n",
    "    ylabel = 'Count'\n",
    "\n",
    "    sns.set_theme(style=\"darkgrid\", font_scale=1.0)\n",
    "\n",
    "    # Create a figure for plotting\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Create a DataFrame to hold all the data for plotting\n",
    "    plot_data = pd.DataFrame()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Filter DataFrame for rows containing the keyword in the 'context' column\n",
    "        keyword_df = df[df['plainpagefulltext'].str.contains(keyword, case=False, na=False)].copy()\n",
    "\n",
    "        # Group by the specified time granularity\n",
    "        if time_granularity == 'year':\n",
    "            grouped_df = keyword_df.groupby('year').size().reset_index(name='count')\n",
    "            grouped_df['time'] = pd.to_datetime(grouped_df['year'].astype(str))\n",
    "        elif time_granularity == 'month':\n",
    "            grouped_df = keyword_df.groupby(['year', 'month']).size().reset_index(name='count')\n",
    "            grouped_df['time'] = pd.to_datetime(grouped_df['year'].astype(str) + '-' + grouped_df['month'].astype(str).str.zfill(2))\n",
    "        elif time_granularity == 'day':\n",
    "            grouped_df = keyword_df.groupby(['year', 'month', 'day']).size().reset_index(name='count')\n",
    "            grouped_df['time'] = pd.to_datetime(grouped_df[['year', 'month', 'day']])\n",
    "        else:\n",
    "            raise ValueError(\"time_granularity must be 'year', 'month', or 'day'\")\n",
    "\n",
    "        # Add a column for the keyword to use in hue and style\n",
    "        grouped_df['keyword'] = keyword\n",
    "\n",
    "        # Append to the plot data\n",
    "        plot_data = pd.concat([plot_data, grouped_df], ignore_index=True)\n",
    "\n",
    "    # Check if plot_data is empty\n",
    "    if plot_data.empty:\n",
    "        print(\"No data available for the given keywords.\")\n",
    "        return\n",
    "\n",
    "    # Sort the intermediary plot_data DataFrame by the 'keyword' first and then 'time' column\n",
    "    plot_data = plot_data.sort_values(by=['keyword', 'time'])\n",
    "\n",
    "    # Format 'time' as year-month for plotting when time_granularity is 'month'\n",
    "    if time_granularity == 'year':\n",
    "        plot_data['time'] = plot_data['time'].dt.strftime('%Y')\n",
    "    elif time_granularity == 'month':\n",
    "        plot_data['time'] = plot_data['time'].dt.strftime('%Y-%m')\n",
    "    elif time_granularity == 'day':\n",
    "        plot_data['time'] = plot_data['time'].dt.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid time granularity specified. Choose from 'year', 'month', or 'day'.\")\n",
    "\n",
    "    # Plot using hue and style for different keywords\n",
    "    sns.lineplot(data=plot_data, x='time', y='count', hue='keyword', style='keyword', markers=True, markersize=8, dashes=False)\n",
    "\n",
    "    # Customize the plot's appearence\n",
    "    plt.xlabel(xlabel, fontsize=13)\n",
    "    plt.ylabel(ylabel, fontsize=13)\n",
    "    plt.title('Search hits for keywords', fontsize=16)\n",
    "    plt.xticks(rotation=65)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "    plt.legend(loc='upper left', fontsize=12, title='Keywords', title_fontsize='14', framealpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function, using the same variable for both `time_granularity` and `xlabel`\n",
    "linegraph_search_hits(\n",
    "    pages, # pass the text data (from the 'context' column of a data frame)\n",
    "    keywords, # Pass the keyword(s) you want to use\n",
    "    time_granularity='month',  # Pass the time granularity\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1sN3MQooM6aOEM_2KM0yNfALYoLYJi9DT",
     "timestamp": 1732190952815
    },
    {
     "file_id": "1Wg2JjNZfl1CKgdy-BXdPJPYGWYOicDNE",
     "timestamp": 1721031400834
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
